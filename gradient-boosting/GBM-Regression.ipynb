{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.datasets import make_regression, make_friedman1\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x, y = make_friedman1(n_features=500, noise=0.5, n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(x, y)\n",
    "sklearn_mse = mean_squared_error(y, reg.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MSELoss(object):\n",
    "    \"\"\"Mean Squared Error loss function\"\"\"\n",
    "    \n",
    "    def get_neg_gradient(self, y_true, y):\n",
    "        return y_true - y\n",
    "    \n",
    "    def __call__(self, y_true, y):\n",
    "        return np.mean((y_true - y) ** 2)\n",
    "    \n",
    "    \n",
    "loss = MSELoss()\n",
    "my_mse = loss(y, reg.predict(x))\n",
    "\n",
    "my_mse == sklearn_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdubovikov/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:93: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.9560358708320256, 2.0291945635469445e-08)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor(object):\n",
    "    \"\"\"Simple Gradient Boosting Regressor implementation\"\"\"\n",
    "    def __init__(self, weak_learner, learning_rate, num_rounds, subsample=1.0, adaptive_learning_rate=False, weak_learner_params_dict={}):\n",
    "        \"\"\"Parameters\n",
    "        -------------\n",
    "        weak_learner: sklearn regressor\n",
    "            a weak learner to boost with\n",
    "        learning_rate: float\n",
    "            a learning rate regularization coefficient, used as initial guess is used with \n",
    "            adaptive_learning_rate mode\n",
    "        num_rounds: int\n",
    "            number of boosting rounds\n",
    "        subsample: float\n",
    "            subsample provideded percentage of observations training set in each boosting \n",
    "            round in the spirit of bagging\n",
    "        adaptive_learning_rate: bool\n",
    "            try to find an optimal learning rate for each boosting round, learning_rate aprameter \n",
    "            is used as an initial guess in this case\n",
    "        weak_learner_params_dict: dict\n",
    "            any additional parameters to be passed to the weak_learner\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_rounds = num_rounds\n",
    "        self.weak_learner = weak_learner\n",
    "        self.weak_learner_params_dict = weak_learner_params_dict\n",
    "        self.adaptive_learning_rate = adaptive_learning_rate\n",
    "        self._loss = MSELoss()\n",
    "        self._gammas = [1]\n",
    "        self._estimators = []\n",
    "        self._subsample_rate = subsample\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fit a Gradient Boosting Regressor\"\"\"\n",
    "        self._fit_base_model(x, y)\n",
    "        preds = self._base_model.predict(x)\n",
    "        \n",
    "        for i in range(0, self.num_rounds):\n",
    "            preds += self._boosting_round(x, y, preds)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict using a trained model\"\"\"\n",
    "        if len(self._estimators) == 0:\n",
    "            raise Exception(\"The model should be trained first. \\\n",
    "                            Please, call the fit method before generating predictions\")\n",
    "            \n",
    "        result = self._estimators[0].predict(x)\n",
    "        for i, estimator in enumerate(self._estimators[1:]):\n",
    "            result += estimator.predict(x) * self._gammas[i + 1]\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def _boosting_round(self, x, y, preds):\n",
    "        \"\"\"Performs a single gradient boosting round\"\"\"\n",
    "        x_ss, y_ss, preds_ss = self._subsample(self._subsample_rate, x, y, preds)\n",
    "        residual = self._loss.get_neg_gradient(y_ss, preds_ss)\n",
    "        \n",
    "        boosting_model = self.weak_learner(**self.weak_learner_params_dict)\n",
    "        boosting_model.fit(x_ss, residual)\n",
    "        old_preds = preds\n",
    "        preds = boosting_model.predict(x)\n",
    "        \n",
    "        if self.adaptive_learning_rate:\n",
    "            gamma = minimize(self._gamma_obj, \n",
    "                             self.learning_rate, \n",
    "                             args=(y, old_preds, preds), \n",
    "                             tol=1e-28, \n",
    "                             method = 'Nelder-Mead').x[0]\n",
    "        else:\n",
    "            gamma = self.learning_rate\n",
    "        \n",
    "        self._estimators.append(boosting_model)\n",
    "        self._gammas.append(gamma)\n",
    "        \n",
    "        return preds * gamma\n",
    "    \n",
    "    def _fit_base_model(self, x, y):\n",
    "        \"\"\"Fits a base weak learner to start boosting with\"\"\"\n",
    "        self._base_model = self.weak_learner(**self.weak_learner_params_dict)\n",
    "        self._base_model.fit(x, y)\n",
    "        self._estimators.append(self._base_model)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _gamma_obj(gamma, y, old_preds, preds):\n",
    "        \"\"\"Objective function used to minimize learning rate when using adaptive mode\"\"\"\n",
    "        return self.loss(y, old_preds + gamma * preds)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _subsample(subsample_rate, x, y, preds):\n",
    "        \"\"\"Generates a random subsample with replacement\"\"\"\n",
    "        sample_size = max(1, subsample_rate * len(x))\n",
    "        mask = np.random.randint(len(x), size=sample_size)\n",
    "        return x[mask, :], y[mask], preds[mask]\n",
    "            \n",
    "gbm = GradientBoostingRegressor(DecisionTreeRegressor, \n",
    "                                learning_rate=0.05, \n",
    "                                num_rounds=100, \n",
    "                                adaptive_learning_rate=False,\n",
    "                                weak_learner_params_dict = {\n",
    "                                    'max_depth': 20\n",
    "                                })\n",
    "gbm.fit(x, y)\n",
    "p = gbm.predict(x)\n",
    "my_mse, loss(y, p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
