{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def get_logger_name(name=None):\n",
    "    root_name = 'notmnist.model'\n",
    "    return root_name if name is None else root_name + '.' + name\n",
    "\n",
    "def setup_logging():\n",
    "    logger = logging.getLogger('notmnist.model')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    ch = logging.StreamHandler()\n",
    "    fh = logging.FileHandler('notmnist-model.log')\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s [%(threadName)s] - %(name)s - %(levelname)s - %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# 'http://yaroslavvb.com/upload/notMNIST/'\n",
    "\n",
    "class notMNISTDataset(object):\n",
    "    def __init__(self, url, filename, num_classes, force=False):\n",
    "        self.log = logging.getLogger(get_logger_name(self.__class__.__name__))\n",
    "        self.url = url\n",
    "        self.num_classes = num_classes\n",
    "        self.filename = filename\n",
    "        \n",
    "        self._maybe_download(force)\n",
    "        self._maybe_extract(force)\n",
    "        \n",
    "    def _maybe_download(self, force=False):\n",
    "        \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "        \n",
    "        def download_progress(count, blockSize, totalSize):\n",
    "            percent = int(count * blockSize * 100 / totalSize)\n",
    "            \n",
    "            if count % 500 == 0:\n",
    "                self.log.info(\"%2d%%\" % percent)\n",
    "        \n",
    "        if force or not os.path.exists(self.filename):\n",
    "            url = self.url + self.filename\n",
    "            self.log.info(\"Downloading %s\" % url)\n",
    "            self.filename, _ = urlretrieve(url, self.filename, reporthook=download_progress)\n",
    "    \n",
    "    def _maybe_extract(self, force=False):\n",
    "        self._data_folder = os.path.splitext(os.path.splitext(self.filename)[0])[0]  # remove .tar.gz\n",
    "        \n",
    "        if os.path.isdir(self._data_folder) and not force:\n",
    "            # You may override by setting force=True.\n",
    "            self.log.info('%s already present - Skipping extraction of %s.' % (self._data_folder, self.filename))\n",
    "        else:\n",
    "            self.log.info('Extracting data for %s. This may take a while. Please wait.' % self._data_folder)\n",
    "            \n",
    "            self._extract_tar(self.filename)\n",
    "            \n",
    "            self._data_folders = [os.path.join(root, d) \n",
    "                                  for d in sorted(os.listdir(root))\n",
    "                                  if os.path.isdir(os.path.join(root, d))]\n",
    "            \n",
    "            if len(self._data_folders) != self.num_classes:\n",
    "                raise Exception(\n",
    "                  'Expected %d folders, one per class. Found %d instead.' % (\n",
    "                    self.num_classes, len(self._data_folders)))\n",
    "                \n",
    "            self.log.info(self._data_folders)\n",
    "            \n",
    "    def read_batch(self, batch_size=32, num_threads=1):\n",
    "        self.log.info(\"Preparing data reader\")\n",
    "        self.filenames = glob.glob(self._data_folder + '/**/*.png', recursive=True)\n",
    "        self.labels = [f.split(os.sep)[1] for f in self.filenames]\n",
    "        \n",
    "        labels_enc = np.array([ord(x) - 65 for x in self.labels])\n",
    "        labels_enc = notMNISTDataset._one_hot(labels_enc)\n",
    "        filename_queue = tf.train.string_input_producer(self.filenames, name='filename_queue')\n",
    "        label_fifo = tf.FIFOQueue(len(self.labels), tf.float32, shapes=[labels_enc.shape[1:]], name='label_fifo')\n",
    "        label_enqueue = label_fifo.enqueue_many([labels_enc])\n",
    "        \n",
    "        # lets add label queue runner to graph collection so it would start with other queue runners\n",
    "        label_qr = tf.train.QueueRunner(label_fifo, [label_enqueue])\n",
    "        tf.add_to_collection(tf.GraphKeys.QUEUE_RUNNERS, label_qr)\n",
    "        \n",
    "        data = self._load_png(filename_queue)\n",
    "        label = label_fifo.dequeue()\n",
    "        \n",
    "        self.log.info(\"Reading batch\")\n",
    "        data_batch, label_batch = tf.train.shuffle_batch([data, label],\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          num_threads=num_threads,\n",
    "                                                          capacity=batch_size * 2,\n",
    "                                                          min_after_dequeue=0,\n",
    "                                                          allow_smaller_final_batch=True, \n",
    "                                                          name='batch_queue')\n",
    "        \n",
    "        \n",
    "        tf.summary.image('image batch', data_batch)\n",
    "        \n",
    "        # return images, tf.reshape(label_batch, [batch_size])\n",
    "        return data_batch, label_batch\n",
    "        \n",
    "    @staticmethod\n",
    "    def _load_png(input_queue):\n",
    "        reader = tf.WholeFileReader()\n",
    "        \n",
    "        _, data = reader.read(input_queue)\n",
    "        data = tf.image.decode_png(data)\n",
    "        data.set_shape([28, 28, 1])\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_tar(filename):\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _one_hot(array):\n",
    "        b = np.zeros((array.size, array.max() + 1))\n",
    "        b[np.arange(array.size), array] = 1\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weights(shape):\n",
    "    w = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(w)\n",
    "\n",
    "def bias(shape):\n",
    "    b = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(b)\n",
    "\n",
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def conv2d_layer(layer_input, filter_shape):\n",
    "    w = weights(filter_shape)\n",
    "    b = bias([filter_shape[-1]])\n",
    "    return tf.nn.relu(conv2d(layer_input, w) + b)\n",
    "\n",
    "def dense_layer(layer_input, filter_shape, activation='relu'):\n",
    "    w = weights(filter_shape)\n",
    "    b = bias([filter_shape[-1]])\n",
    "    linear = tf.matmul(layer_input, w) + b\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        return tf.nn.relu(linear)\n",
    "    elif activation is None:\n",
    "        return linear\n",
    "    else:\n",
    "        raise ValueError('activation %s not supported' % activation)\n",
    "        \n",
    "def simple_convnet(data_batch):\n",
    "    images = tf.to_float(tf.reshape(data_batch, [-1, 28, 28, 1]))\n",
    "    c1 = conv2d_layer(images, [5, 5, 1, 32])\n",
    "    mp1 = max_pool(c1)\n",
    "\n",
    "    c2 = conv2d_layer(mp1, [5, 5, 32, 64])\n",
    "    mp2 = max_pool(c2)\n",
    "\n",
    "    mp2 = tf.reshape(mp2, [-1, 7 * 7 * 64])\n",
    "    d3 = dense_layer(mp2, [7 * 7 * 64, 1024])\n",
    "    d3 = tf.nn.dropout(d3, 0.3)\n",
    "\n",
    "    y_out = dense_layer(d3, [1024, 10], activation=None)\n",
    "    \n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-18 17:07:45,450 [MainThread] - notmnist.model.notMNISTDataset - INFO - notMNIST_large already present - Skipping extraction of notMNIST_large.tar.gz.\n",
      "2017-01-18 17:07:45,452 [MainThread] - notmnist.model.notMNISTDataset - INFO - Preparing data reader\n",
      "2017-01-18 17:07:54,030 [MainThread] - notmnist.model.notMNISTDataset - INFO - Reading batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-54-c52045fd3626>:85 in read_batch.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "Starting session\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n",
      "Running batch reading op\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.operation_timeout_in_ms=100000\n",
    "\n",
    "# load data and prepare ops\n",
    "train_dataset = notMNISTDataset(url='http://yaroslavvb.com/upload/notMNIST/', \n",
    "                    filename='notMNIST_large.tar.gz',\n",
    "                   num_classes=10)\n",
    "\n",
    "# create network and optimizer\n",
    "data_batch, label_batch = train_dataset.read_batch()\n",
    "y_out =  simple_convnet(data_batch)\n",
    "\n",
    "error = tf.nn.softmax_cross_entropy_with_logits(y_out, label_batch)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(error)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_out, 1), tf.argmax(label_batch, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(\"training_accuracy\", accuracy)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('.')\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# run training                                      \n",
    "with tf.Session(config=config) as sess:\n",
    "    print(\"Starting session\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # start queues\n",
    "    coordinator = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "    \n",
    "    while not coordinator.should_stop():  \n",
    "        print(\"Running batch reading op\")\n",
    "        train_step.run()\n",
    "        \n",
    "        # write summaries for Tensorboard\n",
    "        summary = sess.run(merged)\n",
    "        summary_writer.add_summary(summary)\n",
    "        summary_writer.add_graph(sess.graph)\n",
    "\n",
    "    print(\"Requesting stop...\")\n",
    "    coordinator.request_stop()\n",
    "    coordinator.join(threads)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
